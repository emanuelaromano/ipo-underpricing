{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import learning_curve\n",
    "from lightgbm import LGBMClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Issuer Ticker</th>\n",
       "      <th>Issuer Name</th>\n",
       "      <th>Sales - 1 Yr Growth</th>\n",
       "      <th>Profit Margin</th>\n",
       "      <th>Return on Assets</th>\n",
       "      <th>Offer Size (M)</th>\n",
       "      <th>Shares Outstanding (M)</th>\n",
       "      <th>Offer Price</th>\n",
       "      <th>Offer To 1st Close</th>\n",
       "      <th>Market Cap at Offer (M)</th>\n",
       "      <th>...</th>\n",
       "      <th>Instit Owner (Shares Held)</th>\n",
       "      <th>Filing Term Price Range</th>\n",
       "      <th>Priced Range</th>\n",
       "      <th>Industry Sector</th>\n",
       "      <th>Industry Group</th>\n",
       "      <th>Industry Subgroup</th>\n",
       "      <th>Fed Rate</th>\n",
       "      <th>CPI</th>\n",
       "      <th>Consumer Confidence</th>\n",
       "      <th>Unemployment Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000492D US</td>\n",
       "      <td>Pioneer Municipal and Equity I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>430.50</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Financial</td>\n",
       "      <td>Country Funds-Closed-end</td>\n",
       "      <td>Finance-Investment Fund</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.488334</td>\n",
       "      <td>103.8</td>\n",
       "      <td>5.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000708D US</td>\n",
       "      <td>Spirit Finance Corp/Old</td>\n",
       "      <td>128.2</td>\n",
       "      <td>27.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>330.00</td>\n",
       "      <td>63.5068</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.727273</td>\n",
       "      <td>698.57</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.50 - 12.50</td>\n",
       "      <td>Priced Within Range</td>\n",
       "      <td>Financial</td>\n",
       "      <td>REITS</td>\n",
       "      <td>REITS-Diversified</td>\n",
       "      <td>2.04</td>\n",
       "      <td>-0.366492</td>\n",
       "      <td>97.1</td>\n",
       "      <td>5.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0117799D US</td>\n",
       "      <td>Penn Virginia GP Holdings LP</td>\n",
       "      <td>31.5</td>\n",
       "      <td>4.3</td>\n",
       "      <td>2.9</td>\n",
       "      <td>127.65</td>\n",
       "      <td>39.0250</td>\n",
       "      <td>18.5</td>\n",
       "      <td>-2.702703</td>\n",
       "      <td>721.96</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18 - 20</td>\n",
       "      <td>Priced Within Range</td>\n",
       "      <td>Energy</td>\n",
       "      <td>Coal</td>\n",
       "      <td>Coal</td>\n",
       "      <td>5.27</td>\n",
       "      <td>0.148883</td>\n",
       "      <td>91.7</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Issuer Ticker                     Issuer Name  Sales - 1 Yr Growth  \\\n",
       "0   0000492D US  Pioneer Municipal and Equity I                  NaN   \n",
       "1   0000708D US         Spirit Finance Corp/Old                128.2   \n",
       "2   0117799D US    Penn Virginia GP Holdings LP                 31.5   \n",
       "\n",
       "   Profit Margin  Return on Assets  Offer Size (M)  Shares Outstanding (M)  \\\n",
       "0            NaN               NaN          430.50                  0.0000   \n",
       "1           27.7               2.1          330.00                 63.5068   \n",
       "2            4.3               2.9          127.65                 39.0250   \n",
       "\n",
       "   Offer Price  Offer To 1st Close  Market Cap at Offer (M)  ...  \\\n",
       "0         15.0            0.000000                     0.00  ...   \n",
       "1         11.0           12.727273                   698.57  ...   \n",
       "2         18.5           -2.702703                   721.96  ...   \n",
       "\n",
       "  Instit Owner (Shares Held) Filing Term Price Range         Priced Range  \\\n",
       "0                        NaN                     NaN                  NaN   \n",
       "1                        NaN           10.50 - 12.50  Priced Within Range   \n",
       "2                        NaN                 18 - 20  Priced Within Range   \n",
       "\n",
       "   Industry Sector            Industry Group        Industry Subgroup  \\\n",
       "0        Financial  Country Funds-Closed-end  Finance-Investment Fund   \n",
       "1        Financial                     REITS        REITS-Diversified   \n",
       "2           Energy                      Coal                     Coal   \n",
       "\n",
       "   Fed Rate       CPI Consumer Confidence Unemployment Rate  \n",
       "0      0.94  0.488334               103.8               5.7  \n",
       "1      2.04 -0.366492                97.1               5.4  \n",
       "2      5.27  0.148883                91.7               4.4  \n",
       "\n",
       "[3 rows x 26 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../build_dataset/output_csv/bloomberg_data.csv\")\n",
    "df = df.dropna(subset=['Offer To 1st Close'])\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_features(df):\n",
    "    \"\"\"\n",
    "    Convert the 'Trade Date (US)' column to separate columns for \n",
    "    month, day, and year, and drop the original 'Trade Date (US)' column.\n",
    "\n",
    "    Also, convert Offer To 1st Close to a binary classification (Underpriced\n",
    "    column) and drop original column.\n",
    "    \"\"\"\n",
    "    df['Trade Date (US)'] = pd.to_datetime(df['Trade Date (US)'])\n",
    "    df['Trade Month'] = df['Trade Date (US)'].dt.month\n",
    "    df['Trade Day'] = df['Trade Date (US)'].dt.day\n",
    "    df['Trade Year'] = df['Trade Date (US)'].dt.year\n",
    "    df.drop(columns=['Trade Date (US)'], inplace=True)\n",
    "    df['Underpriced'] = df['Offer To 1st Close'].apply(lambda x: 1 if x < 0 else 0)\n",
    "    df.drop(columns=['Offer To 1st Close'], inplace=True)\n",
    "    return df\n",
    "\n",
    "df_convert_features = convert_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(columns,df):\n",
    "    \"\"\"\n",
    "    Drop the specified columns from the DataFrame.\n",
    "    \"\"\"\n",
    "    df = df.drop(columns=columns)\n",
    "    return df\n",
    "\n",
    "df_drop_columns = drop_columns(['Issuer Ticker',\n",
    "                                'Issuer Name',\n",
    "                                'Filing Term Price Range',\n",
    "                                'cusip',\n",
    "                                'Priced Range'],\n",
    "                                df_convert_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding(df):\n",
    "    \"\"\"\n",
    "    Split the DataFrame into development and testing sets, using stratified sampling. \n",
    "    \n",
    "    Fill missing values with the mode for categorical \n",
    "    columns and the mean for numerical columns.\n",
    "\n",
    "    Encode columns in the DataFrame: \n",
    "\n",
    "      - Categorical columns are encoded using OneHotEncoder.\n",
    "      - Numerical columns are scaled using StandardScaler.\n",
    "      - Ordinal columns are encoded using OrdinalEncoder.\n",
    "\n",
    "    Create df_dev and df_test using the transformed features.\n",
    "    \"\"\"\n",
    "\n",
    "    X = df.drop(columns=['Underpriced'])\n",
    "    y = df['Underpriced']\n",
    "\n",
    "    X_dev, X_test, y_dev, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    for col in X_dev.columns:\n",
    "        if X_dev[col].dtype == 'object':\n",
    "            mode = X_dev[col].mode()[0]\n",
    "            X_dev[col] = X_dev[col].fillna(mode)\n",
    "            X_test[col] = X_test[col].fillna(mode)\n",
    "        else:\n",
    "            mean = X_dev[col].mean()\n",
    "            X_dev[col] = X_dev[col].fillna(mean)\n",
    "            X_test[col] = X_test[col].fillna(mean)\n",
    "\n",
    "\n",
    "    oe_columns = ['Trade Month', 'Trade Day', 'Trade Year'] \n",
    "    ohe_columns = ['Industry Sector', 'Industry Group', 'Industry Subgroup']\n",
    "    ss_columns = [col for col in X.select_dtypes(exclude=['object']).columns if col not in oe_columns]\n",
    "\n",
    "    oe = OrdinalEncoder()\n",
    "    ohe = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "    ss = StandardScaler()\n",
    "\n",
    "    preprocess = ColumnTransformer(transformers=[\n",
    "        ('ohe', ohe, ohe_columns),\n",
    "        ('ss', ss, ss_columns),\n",
    "        ('oe', oe, oe_columns)\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "    transformed_dev = preprocess.fit_transform(X_dev)\n",
    "    transformed_test = preprocess.transform(X_test)\n",
    "\n",
    "    features = preprocess.get_feature_names_out()\n",
    "\n",
    "    df_dev = pd.DataFrame(transformed_dev, columns=features)\n",
    "    df_test = pd.DataFrame(transformed_test, columns=features)\n",
    "\n",
    "    df_dev['Underpriced'] = y_dev.reset_index(drop=True)\n",
    "    df_test['Underpriced'] = y_test.reset_index(drop=True)\n",
    "\n",
    "    assert not df_dev.isnull().any().any(), \"Missing values found in development set\"\n",
    "    assert not df_test.isnull().any().any(), \"Missing values found in test set\"\n",
    "    assert len(df_dev.columns) == len(df_test.columns), \"Mismatch in number of features\"\n",
    "\n",
    "    return df_dev, df_test\n",
    "\n",
    "df_dev_encoding, df_test_encoding = encoding(df_drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 29 correlated features.\n"
     ]
    }
   ],
   "source": [
    "def get_correlation(df_dev, df_test, outfile):\n",
    "    \"\"\"\n",
    "    Check correlation in the X_dev column and drop highly correlated features.\n",
    "    \"\"\"\n",
    "    # Calculate correlation on training data only\n",
    "    X_dev = df_dev.drop(columns=['Underpriced'])\n",
    "    corr_matrix = X_dev.corr().abs()\n",
    "    \n",
    "    # Find highly correlated features\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] >= 0.9)]\n",
    "    \n",
    "    # Apply the same feature selection to both sets\n",
    "    X_dev_reduced = X_dev.drop(columns=to_drop)\n",
    "    X_test_reduced = df_test.drop(columns=['Underpriced']).drop(columns=to_drop)\n",
    "\n",
    "    df_dev = pd.concat([X_dev_reduced, df_dev['Underpriced'].reset_index(drop=True)], axis=1)\n",
    "    df_test = pd.concat([X_test_reduced, df_test['Underpriced'].reset_index(drop=True)], axis=1)\n",
    "\n",
    "    print(f\"Removed {len(to_drop)} correlated features.\")\n",
    "    df_test.to_csv(outfile, index=False)\n",
    "\n",
    "    return df_dev, df_test\n",
    "\n",
    "df_dev_correlation, df_test_correlation = get_correlation(df_dev_encoding, df_test_encoding, './output_csv/Final_Output_Class_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_distribution(df_dev_correlation):\n",
    "    \"\"\"\n",
    "    Displaying pie plot to show target data imbalance.\n",
    "    \"\"\"\n",
    "    df = df_dev_correlation.copy()\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    underpriced_counts = df['Underpriced'].value_counts()\n",
    "    display(underpriced_counts)\n",
    "    labels = ['Underpriced', 'Not Underpriced']\n",
    "    colors = ['lightblue', '#212351']\n",
    "\n",
    "    wedges, texts, autotexts = plt.pie(\n",
    "        underpriced_counts, labels=labels, colors=colors,\n",
    "        autopct='%1.1f%%', textprops={'color': 'black'}\n",
    "    )\n",
    "\n",
    "    for i, autotext in enumerate(autotexts):\n",
    "        if colors[i] == '#212351':\n",
    "            autotext.set_color('white')\n",
    "\n",
    "    plt.title('Classification Task: Underpriced vs Not Underpriced (Before Applying SMOTE)')\n",
    "    plt.show()\n",
    "    \n",
    "display_distribution(df_dev_correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkimbalance(df, outfile):\n",
    "    \"\"\"\n",
    "    Checking imbalance in the target data. \n",
    "\n",
    "    If imbalance, resample using SMOTE.\n",
    "    \"\"\"\n",
    "    X_dev = df.drop(columns=['Underpriced'])\n",
    "    y_dev = df['Underpriced']\n",
    "\n",
    "    label_count_norm = df['Underpriced'].value_counts(normalize=True)\n",
    "    label_count = df['Underpriced'].value_counts()\n",
    "    print(f\"Class distribution (normalized):\\n{label_count_norm}\\n\")\n",
    "    print(f\"Class distribution:\\n{label_count}\\n\")\n",
    "\n",
    "\n",
    "    imbalance = abs(label_count_norm[0] - 0.5) > 0.1\n",
    "\n",
    "    if imbalance:\n",
    "        print('The data is imbalanced. Applying SMOTE...')\n",
    "        sampler = SMOTE(random_state=42)\n",
    "        X_resampled, y_resampled = sampler.fit_resample(X_dev, y_dev)\n",
    "        df_final = pd.concat([X_resampled, y_resampled], axis=1)\n",
    "        df_final.to_csv(outfile, index=False)\n",
    "        print(f'The initial dataset had {y_dev.shape[0]} rows')\n",
    "        print(f'The resampled dataset has {y_resampled.shape[0]} rows')\n",
    "        print(f'Resampled feature matrix shape: {X_resampled.shape}')\n",
    "        return df_final\n",
    "    else:\n",
    "        print('The data is balanced. No resampling applied.')\n",
    "        return df\n",
    "    \n",
    "df_dev_resampled = checkimbalance(df_dev_correlation, './output_csv/Final_Output_Class_dev.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, target_col='Underpriced'):\n",
    "    \"\"\"Preprocess the data with feature engineering and scaling\"\"\"\n",
    "    X = df.drop(columns=target_col)\n",
    "    y = df[target_col]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_enhanced_model_params():\n",
    "    \"\"\"Return enhanced classification models with wider search space\"\"\"\n",
    "    return {\n",
    "        \"Logistic Regression\": {\n",
    "            \"model\": LogisticRegression(solver='liblinear', random_state=42),\n",
    "            \"params\": {\n",
    "                \"C\": [0.01, 0.1, 1, 10, 100]\n",
    "            }\n",
    "        },\n",
    "        \"Ridge Classifier\": {\n",
    "            \"model\": RidgeClassifier(),\n",
    "            \"params\": {\n",
    "                \"alpha\": [0.1, 1.0, 10.0]\n",
    "            }\n",
    "        },\n",
    "        \"Support Vector Classifier\": {\n",
    "            \"model\": SVC(probability=True, random_state=42),\n",
    "            \"params\": {\n",
    "                \"kernel\": [\"rbf\", \"poly\"],\n",
    "                \"C\": [0.1, 1, 10],\n",
    "                \"gamma\": [\"scale\", \"auto\"]\n",
    "            }\n",
    "        },\n",
    "        \"LightGBM Classifier\": {\n",
    "            \"model\": LGBMClassifier(random_state=42, n_jobs=-1),\n",
    "            \"params\": {\n",
    "                \"n_estimators\": [100, 200, 500],\n",
    "                \"learning_rate\": [0.001, 0.01, 0.1],\n",
    "                \"max_depth\": [3, 5, 7, -1],\n",
    "                \"num_leaves\": [15, 31, 63],\n",
    "                \"subsample\": [0.8, 0.9, 1.0],\n",
    "                \"verbosity\": [-1]\n",
    "            }\n",
    "        },\n",
    "        \"Random Forest Classifier\": {\n",
    "            \"model\": RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "            \"params\": {\n",
    "                \"n_estimators\": [100, 200],\n",
    "                \"max_depth\": [None, 10, 20],\n",
    "                \"max_features\": [\"sqrt\", \"log2\"]\n",
    "            }\n",
    "        },\n",
    "        \"K-Nearest Neighbors\": {\n",
    "            \"model\": KNeighborsClassifier(),\n",
    "            \"params\": {\n",
    "                \"n_neighbors\": [3, 5, 7],\n",
    "                \"weights\": [\"uniform\", \"distance\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(model_params, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Evaluate classification models with multiple metrics\"\"\"\n",
    "    results = []\n",
    "\n",
    "    total_models = len(model_params)\n",
    "    print(f\"Evaluating {total_models} models...\\n\")\n",
    "\n",
    "    for i, (name, mp) in enumerate(model_params.items(), 1):\n",
    "        print(f\"[{i}/{total_models}] Training and evaluating: {name}...\", end=\" \")\n",
    "\n",
    "        try:\n",
    "            if mp[\"params\"]:\n",
    "                grid = GridSearchCV(\n",
    "                    mp[\"model\"], \n",
    "                    mp[\"params\"], \n",
    "                    cv=5, \n",
    "                    scoring='f1', \n",
    "                    n_jobs=-1,\n",
    "                    verbose=0\n",
    "                )\n",
    "                grid.fit(X_train, y_train)\n",
    "                best_model = grid.best_estimator_\n",
    "                print(\"âœ“ (GridSearch complete)\")\n",
    "            else:\n",
    "                best_model = mp[\"model\"]\n",
    "                best_model.fit(X_train, y_train)\n",
    "                print(\"âœ“ (No tuning needed)\")\n",
    "            \n",
    "            # Cross-validation scores\n",
    "            cv_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='f1')\n",
    "            \n",
    "            y_pred = best_model.predict(X_val)\n",
    "            acc = accuracy_score(y_val, y_pred)\n",
    "            f1 = f1_score(y_val, y_pred, zero_division=0)\n",
    "            precision = precision_score(y_val, y_pred, zero_division=0)\n",
    "            recall = recall_score(y_val, y_pred, zero_division=0)\n",
    "            \n",
    "            results.append({\n",
    "                \"Model\": name,\n",
    "                \"Accuracy\": acc,\n",
    "                \"F1 Score\": f1,\n",
    "                \"Precision\": precision,\n",
    "                \"Recall\": recall,\n",
    "                \"CV F1 Mean\": cv_scores.mean(),\n",
    "                \"CV F1 Std\": cv_scores.std()\n",
    "            })\n",
    "            \n",
    "            # Plot learning curves\n",
    "            train_sizes, train_scores, val_scores = learning_curve(\n",
    "                best_model, X_train, y_train, cv=5, scoring='f1',\n",
    "                train_sizes=np.linspace(0.1, 1.0, 10)\n",
    "            )\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(train_sizes, train_scores.mean(axis=1), label='Training score')\n",
    "            plt.plot(train_sizes, val_scores.mean(axis=1), label='Cross-validation score')\n",
    "            plt.fill_between(train_sizes, \n",
    "                           train_scores.mean(axis=1) - train_scores.std(axis=1),\n",
    "                           train_scores.mean(axis=1) + train_scores.std(axis=1),\n",
    "                           alpha=0.1)\n",
    "            plt.fill_between(train_sizes,\n",
    "                           val_scores.mean(axis=1) - val_scores.std(axis=1),\n",
    "                           val_scores.mean(axis=1) + val_scores.std(axis=1),\n",
    "                           alpha=0.1)\n",
    "            plt.title(f'Learning Curves - {name}')\n",
    "            plt.xlabel('Training examples')\n",
    "            plt.ylabel('F1 Score')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "            \n",
    "            # Plot feature importance for tree-based models\n",
    "            if hasattr(best_model, 'feature_importances_'):\n",
    "                importances = best_model.feature_importances_\n",
    "                indices = np.argsort(importances)[::-1]\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.title(f'Feature Importances - {name}')\n",
    "                plt.bar(range(X_train.shape[1]), importances[indices])\n",
    "                plt.xticks(range(X_train.shape[1]), X_train.columns[indices], rotation=90)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Error with {name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    top5_models_df = results_df.sort_values(by=\"F1 Score\", ascending=False).head(5).reset_index(drop=True)\n",
    "    \n",
    "    return results_df, top5_models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_column_names(df):\n",
    "    df.columns = df.columns.str.replace(r\"[^\\w\\d_]+\", \"_\", regex=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    try:\n",
    "        df_dev = pd.read_csv('./output_csv/Final_Output_Class_dev.csv')\n",
    "        X, y = split_data(df_dev)\n",
    "        X = clean_column_names(X)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        model_params = get_enhanced_model_params()\n",
    "        results_df, top5_models_df = evaluate_models(model_params, X_train, y_train, X_val, y_val)\n",
    "        \n",
    "        print(\"\\nTop 5 Models:\")\n",
    "        print(top5_models_df)\n",
    "        \n",
    "        results_df.to_csv('./output_csv/classification_results.csv', index=False)\n",
    "        top5_models_df.to_csv('./output_csv/top5_classification_models.csv', index=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
