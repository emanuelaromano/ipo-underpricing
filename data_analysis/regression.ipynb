{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../build_dataset/output_csv/bloomberg_data.csv')\n",
    "df = df.dropna(subset=['Offer To 1st Close'])\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rolling_ipos(df):\n",
    "    df['Trade Date (US)'] = pd.to_datetime(df['Trade Date (US)'])\n",
    "    df = df.sort_values('Trade Date (US)')\n",
    "    df['Prev_5_IPOs_Avg_Return'] = df['Offer To 1st Close'].rolling(window=5, min_periods=1).mean().shift(1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_rolling_ipos = add_rolling_ipos(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_trade_date(df):\n",
    "    \"\"\"\n",
    "    Convert the 'Trade Date (US)' column to separate columns for \n",
    "    month, day, and year, and drop the original 'Trade Date (US)' column.\n",
    "    \"\"\"\n",
    "    df['Trade Date (US)'] = pd.to_datetime(df['Trade Date (US)'])\n",
    "    df['Trade Month'] = df['Trade Date (US)'].dt.month\n",
    "    df['Trade Day'] = df['Trade Date (US)'].dt.day\n",
    "    df['Trade Year'] = df['Trade Date (US)'].dt.year\n",
    "    df.drop(columns=['Trade Date (US)'], inplace=True)\n",
    "    return df\n",
    "\n",
    "df_convert_trade_date = convert_trade_date(df_rolling_ipos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(columns,df):\n",
    "    \"\"\"\n",
    "    Drop the specified columns from the DataFrame.\n",
    "    \"\"\"\n",
    "    df = df.drop(columns=columns)\n",
    "    return df\n",
    "\n",
    "df_drop_columns = drop_columns(['Issuer Ticker',\n",
    "                                'Issuer Name',\n",
    "                                'Filing Term Price Range',\n",
    "                                'cusip',\n",
    "                                'Priced Range'],\n",
    "                                df_convert_trade_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding(df):\n",
    "    \"\"\"\n",
    "    Split the DataFrame into development and testing sets.\n",
    "\n",
    "    Fill missing values with the mode for categorical\n",
    "    columns and the mean for numerical columns.\n",
    "\n",
    "    Encode columns in the DataFrame:\n",
    "\n",
    "      - Categorical columns are encoded using OneHotEncoder.\n",
    "      - Numerical columns are scaled using StandardScaler.\n",
    "      - Ordinal columns are encoded using OrdinalEncoder.\n",
    "\n",
    "    Create df_dev and df_test using the transformed features.\n",
    "    \"\"\"\n",
    "\n",
    "    X = df.drop(columns=['Offer To 1st Close'])\n",
    "    y = df['Offer To 1st Close']\n",
    "\n",
    "    X_dev, X_test, y_dev, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    for col in X_dev.columns:\n",
    "        if X_dev[col].dtype == 'object':\n",
    "            X_dev[col] = X_dev[col].fillna(X_dev[col].mode()[0])\n",
    "            X_test[col] = X_test[col].fillna(X_dev[col].mode()[0])\n",
    "        else:\n",
    "            X_dev[col] = X_dev[col].fillna(X_dev[col].mean())\n",
    "            X_test[col] = X_test[col].fillna(X_test[col].mean())\n",
    "\n",
    "    assert not X_dev.isnull().any().any(), \"Missing values found in development set\"\n",
    "    assert not X_test.isnull().any().any(), \"Missing values found in test set\"\n",
    "\n",
    "    oe_columns = ['Trade Month', 'Trade Day', 'Trade Year']\n",
    "    ohe_columns = ['Industry Sector', 'Industry Group', 'Industry Subgroup']\n",
    "    ss_columns = [col for col in X.select_dtypes(exclude=['object']).columns if col not in oe_columns]\n",
    "\n",
    "    oe = OrdinalEncoder()\n",
    "    ohe = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "    ss = StandardScaler()\n",
    "\n",
    "    preprocess = ColumnTransformer(transformers=[\n",
    "        ('ohe', ohe, ohe_columns),\n",
    "        ('ss', ss, ss_columns),\n",
    "        ('oe', oe, oe_columns)\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "    transformed_dev = preprocess.fit_transform(X_dev)\n",
    "    transformed_test = preprocess.transform(X_test)\n",
    "\n",
    "    features = preprocess.get_feature_names_out()\n",
    "\n",
    "    df_dev = pd.DataFrame(transformed_dev, columns=features)\n",
    "    df_test = pd.DataFrame(transformed_test, columns=features)\n",
    "\n",
    "    df_dev['Offer To 1st Close'] = y_dev.reset_index(drop=True)\n",
    "    df_test['Offer To 1st Close'] = y_test.reset_index(drop=True)\n",
    "\n",
    "    assert not df_dev.isna().any().any(), \"Missing values found in development set\"\n",
    "    assert not df_test.isna().any().any(), \"Missing values found in test set\"\n",
    "    assert len(df_dev.columns) == len(df_test.columns), \"Mismatch in number of features\"\n",
    "    assert 'Offer To 1st Close' in df_dev.columns and 'Offer To 1st Close' in df_test.columns, \"Target column missing\"\n",
    "\n",
    "    return df_dev, df_test\n",
    "\n",
    "\n",
    "df_dev_encoding, df_test_encoding = encoding(df_drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_feature_importance(df_dev):\n",
    "    \"\"\"\n",
    "    Calculate feature importance using a simple model\n",
    "    \"\"\"\n",
    "    X_dev = df_dev.drop(columns=['Offer To 1st Close'])\n",
    "    y_dev = df_dev['Offer To 1st Close']\n",
    "    \n",
    "    rf = RandomForestRegressor(random_state=42)\n",
    "    rf.fit(X_dev, y_dev)\n",
    "    importances = pd.Series(rf.feature_importances_, index=X_dev.columns)\n",
    "    print(\"\\nFeature Importances:\")\n",
    "    print(importances.sort_values(ascending=False).head(10))\n",
    "\n",
    "show_feature_importance(df_dev_encoding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correlation(df_dev, df_test):\n",
    "    \"\"\"\n",
    "    Check correlation in the X_dev column and drop highly correlated features.\n",
    "    \"\"\"\n",
    "    X_dev = df_dev.drop(columns=['Offer To 1st Close'])\n",
    "    y_dev = df_dev['Offer To 1st Close']\n",
    "\n",
    "    X_test = df_test.drop(columns=['Offer To 1st Close'])\n",
    "    y_test = df_test['Offer To 1st Close']\n",
    "\n",
    "    corr_matrix = X_dev.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] >= 0.9)]\n",
    "\n",
    "    X_reduced = X_dev.drop(columns=to_drop)\n",
    "    X_reduced_test = X_test.drop(columns=to_drop)\n",
    "\n",
    "    df_dev = pd.concat([X_reduced, y_dev.reset_index(drop=True)], axis=1)\n",
    "    df_test = pd.concat([X_reduced_test, y_test.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    print(f\"Removed {len(to_drop)} correlated features.\")\n",
    "\n",
    "    df_test.to_csv('./output_csv/Final_Output_Reg_test.csv', index=False)\n",
    "\n",
    "    return df_dev, df_test\n",
    "\n",
    "df_dev_correlation, df_test_correlation = get_correlation(df_dev_encoding, df_test_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outlier(df):\n",
    "    \"\"\"\n",
    "    Remove outliers using both IQR on target variable and standardized residuals\n",
    "    \"\"\"\n",
    "    label = df['Offer To 1st Close']\n",
    "    Q1 = label.quantile(0.25)\n",
    "    Q3 = label.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    df_filtered = df[(label >= lower_bound) & (label <= upper_bound)]\n",
    "    \n",
    "    X = df_filtered.drop(columns=['Offer To 1st Close'])\n",
    "    y = df_filtered['Offer To 1st Close']\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    residuals = y - y_pred\n",
    "    std_residuals = (residuals - residuals.mean()) / residuals.std()\n",
    "    \n",
    "    df_filtered = df_filtered[abs(std_residuals) <= 3]\n",
    "    \n",
    "    print(f\"Removed outliers.\")\n",
    "    print(f\"Original count: {len(df)}\")\n",
    "    print(f\"After IQR filtering: {len(df_filtered)}\")\n",
    "    print(f\"After residual filtering: {len(df_filtered)}\")\n",
    "    \n",
    "    return df_filtered, (lower_bound, upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkoutliers(df, outfile):\n",
    "    \"\"\"\n",
    "    Print distribution of the target variable and check for outliers.\n",
    "    \"\"\"\n",
    "    label = df['Offer To 1st Close']\n",
    "\n",
    "    df_stat = pd.DataFrame([{\n",
    "        'Minimum': label.min(),\n",
    "        'Q1': label.quantile(0.25),\n",
    "        'Median': label.median(),\n",
    "        'Q3': label.quantile(0.75),\n",
    "        'Maximum': label.max(),\n",
    "        'Mean': label.mean(),\n",
    "        'Standard deviation': label.std()\n",
    "    }])\n",
    "\n",
    "    display(df_stat.transpose().rename(columns={0: 'Values'}))\n",
    "    print('\\nChecking for outliers...')\n",
    "    \n",
    "    df_filtered, bounds = remove_outlier(df)\n",
    "    df_filtered.to_csv(outfile, index=False)\n",
    "    return df_filtered, bounds\n",
    "\n",
    "df_dev_filtered, bounds = checkoutliers(df_dev_correlation, './output_csv/Final_Output_Reg_dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_distribution(df_dev_filtered, df_dev_correlation):\n",
    "    \"\"\"\n",
    "    Display distribution of target variable before and after removing outliers.\n",
    "    \"\"\"\n",
    "    df_no_outliers = df_dev_filtered.copy()\n",
    "    df_with_outliers = df_dev_correlation.copy()\n",
    "\n",
    "    fig, axs = plt.subplots(ncols=2, figsize=(15, 10))\n",
    "\n",
    "    sns.boxplot(data=df_with_outliers, y='Offer To 1st Close', color='lightblue', ax=axs[0])\n",
    "    axs[0].set_title('With Outliers')\n",
    "    axs[0].set_xlabel('Offer to 1st Close')\n",
    "\n",
    "    sns.boxplot(data=df_no_outliers, y='Offer To 1st Close', color='lightblue', ax=axs[1])\n",
    "    axs[1].set_title('Without Outliers')\n",
    "    axs[1].set_xlabel('Offer to 1st Close')\n",
    "\n",
    "    fig.suptitle('Regression Task: Distribution of Offer to 1st Close', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "display_distribution(df_dev_filtered, df_dev_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_dimensionality_reduction(df_dev, df_test, n_components=0.95):\n",
    "    \"\"\"\n",
    "    Apply dimensionality reduction techniques to the data.\n",
    "    \"\"\"\n",
    "    X_dev = df_dev.drop(columns=['Offer To 1st Close'])\n",
    "    y_dev = df_dev['Offer To 1st Close']\n",
    "    \n",
    "    X_test = df_test.drop(columns=['Offer To 1st Close'])\n",
    "    y_test = df_test['Offer To 1st Close']\n",
    "    \n",
    "    pca = PCA(n_components=n_components, random_state=42)\n",
    "    X_dev_pca = pca.fit_transform(X_dev)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "\n",
    "    print(f\"\\nNumber of components after PCA: {pca.n_components_}\")\n",
    "    print(f\"Explained variance ratio: {pca.explained_variance_ratio_.sum():.2f}\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.title('PCA Explained Variance')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    df_dev_reduced = pd.DataFrame(X_dev_pca, columns=[f'PC{i+1}' for i in range(X_dev_pca.shape[1])])\n",
    "    df_test_reduced = pd.DataFrame(X_test_pca, columns=[f'PC{i+1}' for i in range(X_test_pca.shape[1])])\n",
    "    \n",
    "    df_dev_reduced['Offer To 1st Close'] = y_dev.reset_index(drop=True)\n",
    "    df_test_reduced['Offer To 1st Close'] = y_test.reset_index(drop=True)\n",
    "    \n",
    "    df_dev_reduced.to_csv('./output_csv/Final_Output_Reg_dev.csv', index=False)\n",
    "    df_test_reduced.to_csv('./output_csv/Final_Output_Reg_test.csv', index=False)\n",
    "    \n",
    "    return df_dev_reduced, df_test_reduced\n",
    "\n",
    "df_dev_reduced, df_test_reduced = apply_dimensionality_reduction(df_dev_filtered, df_test_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, target_col='Offer To 1st Close'):\n",
    "    \"\"\"\n",
    "    Preprocess the data with feature engineering and scaling\n",
    "    \"\"\"\n",
    "    X = df.drop(columns=target_col)\n",
    "    y = df[target_col]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_enhanced_model_params():\n",
    "    \"\"\"Return enhanced model parameters with wider search space\"\"\"\n",
    "    return {\n",
    "        \"Linear Regression\": {\n",
    "            \"model\": LinearRegression(n_jobs=-1),\n",
    "            \"params\": {}\n",
    "        },\n",
    "        \"Ridge Regression\": {\n",
    "            \"model\": Ridge(random_state=42, max_iter=10000),\n",
    "            \"params\": {\n",
    "                \"alpha\": [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "            }\n",
    "        },\n",
    "        \"Lasso Regression\": {\n",
    "            \"model\": Lasso(random_state=42, max_iter=10000),\n",
    "            \"params\": {\n",
    "                \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "            }\n",
    "        },\n",
    "        \"ElasticNet Regression\": {\n",
    "            \"model\": ElasticNet(random_state=42, max_iter=10000),\n",
    "            \"params\": {\n",
    "                \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10],\n",
    "                \"l1_ratio\": [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "            }\n",
    "        },\n",
    "        \"Support Vector Regressor\": {\n",
    "            \"model\": SVR(),\n",
    "            \"params\": {\n",
    "                \"kernel\": [\"rbf\", \"poly\"],\n",
    "                \"C\": [0.1, 1, 10],\n",
    "                \"gamma\": [\"scale\", \"auto\"]\n",
    "            }\n",
    "        },\n",
    "        \"LightGBM\": {\n",
    "            \"model\": LGBMRegressor(\n",
    "                random_state=42,\n",
    "                force_col_wise=True,  \n",
    "                min_data_in_leaf=10,  \n",
    "                feature_fraction=0.8,  \n",
    "                bagging_fraction=0.8, \n",
    "                bagging_freq=1,     \n",
    "                verbose=-1         \n",
    "            ),\n",
    "            \"params\": {\n",
    "                \"n_estimators\": [100, 200],\n",
    "                \"max_depth\": [5, 7],\n",
    "                \"learning_rate\": [0.01, 0.05],\n",
    "                \"num_leaves\": [20, 31],\n",
    "                \"reg_alpha\": [0.01, 0.1],     \n",
    "                \"reg_lambda\": [0.01, 0.1]\n",
    "            }\n",
    "        },\n",
    "        \"Random Forest (Light)\": {\n",
    "            \"model\": RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "            \"params\": {\n",
    "                \"n_estimators\": [50, 100],\n",
    "                \"max_depth\": [3, 5],\n",
    "                \"min_samples_split\": [5, 10]\n",
    "            }\n",
    "        },\n",
    "        \"Gradient Boosting\": {\n",
    "            \"model\": GradientBoostingRegressor(random_state=42),\n",
    "            \"params\": {\n",
    "                \"n_estimators\": [50, 100],\n",
    "                \"max_depth\": [3, 5],\n",
    "                \"learning_rate\": [0.01, 0.05],\n",
    "                \"subsample\": [0.8, 1.0],\n",
    "                \"min_samples_split\": [5, 10]\n",
    "            }\n",
    "        },\n",
    "        \"Extra Trees\": {\n",
    "            \"model\": ExtraTreesRegressor(random_state=42, n_jobs=-1),\n",
    "            \"params\": {\n",
    "                \"n_estimators\": [50, 100],\n",
    "                \"max_depth\": [3, 5],\n",
    "                \"min_samples_split\": [5, 10]\n",
    "            }\n",
    "        },\n",
    "        \"Kernel Ridge\": {\n",
    "            \"model\": KernelRidge(kernel='rbf'),\n",
    "            \"params\": {\n",
    "                \"alpha\": [0.001, 0.01, 0.1, 1.0],\n",
    "                \"gamma\": [0.001, 0.01, 0.1, 1.0]\n",
    "            }\n",
    "        },\n",
    "        \"Neural Network\": {\n",
    "            \"model\": MLPRegressor(\n",
    "                random_state=42,\n",
    "                max_iter=2000,          \n",
    "                early_stopping=True,    \n",
    "                validation_fraction=0.1, \n",
    "                n_iter_no_change=50,    \n",
    "                batch_size='auto',      \n",
    "                learning_rate='adaptive' \n",
    "            ),\n",
    "            \"params\": {\n",
    "                \"hidden_layer_sizes\": [(100,), (100, 50), (50, 25)],\n",
    "                \"learning_rate_init\": [0.001, 0.005],\n",
    "                \"alpha\": [0.0001, 0.001, 0.01],\n",
    "                \"activation\": ['relu', 'tanh']\n",
    "            }\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(model_params, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Evaluate models with RMSE as the primary metric\"\"\"\n",
    "    results = []\n",
    "\n",
    "    total_models = len(model_params)\n",
    "    print(f\"Evaluating {total_models} models...\\n\")\n",
    "\n",
    "    for i, (name, mp) in enumerate(model_params.items(), 1):\n",
    "        print(f\"[{i}/{total_models}] Training and evaluating: {name}...\", end=\" \")\n",
    "\n",
    "        try:\n",
    "            if mp[\"params\"]:\n",
    "                grid = GridSearchCV(\n",
    "                    mp[\"model\"],\n",
    "                    mp[\"params\"],\n",
    "                    cv=5,\n",
    "                    scoring='neg_root_mean_squared_error',\n",
    "                    n_jobs=-1,\n",
    "                    verbose=0\n",
    "                )\n",
    "                grid.fit(X_train, y_train)\n",
    "                best_model = grid.best_estimator_\n",
    "                print(\"✓ (GridSearch complete)\")\n",
    "            else:\n",
    "                best_model = mp[\"model\"]\n",
    "                best_model.fit(X_train, y_train)\n",
    "                print(\"✓ (No tuning needed)\")\n",
    "\n",
    "            cv_scores = cross_val_score(best_model, X_train, y_train, cv=5,\n",
    "                                      scoring='neg_root_mean_squared_error')\n",
    "            cv_scores = -cv_scores\n",
    "\n",
    "            y_pred = best_model.predict(X_val)\n",
    "            rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "            mae = mean_absolute_error(y_val, y_pred)\n",
    "            r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "            results.append({\n",
    "                \"Model\": name,\n",
    "                \"RMSE\": rmse,\n",
    "                \"MAE\": mae,\n",
    "                \"R² Score\": r2,\n",
    "                \"CV RMSE Mean\": cv_scores.mean(),\n",
    "                \"CV RMSE Std\": cv_scores.std()\n",
    "            })\n",
    "\n",
    "            train_sizes, train_scores, val_scores = learning_curve(\n",
    "                best_model, X_train, y_train, cv=5,\n",
    "                scoring='neg_root_mean_squared_error',\n",
    "                train_sizes=np.linspace(0.1, 1.0, 10)\n",
    "            )\n",
    "\n",
    "            train_scores = -train_scores\n",
    "            val_scores = -val_scores\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(train_sizes, train_scores.mean(axis=1), label='Training RMSE')\n",
    "            plt.plot(train_sizes, val_scores.mean(axis=1), label='Cross-validation RMSE')\n",
    "            plt.fill_between(train_sizes,\n",
    "                           train_scores.mean(axis=1) - train_scores.std(axis=1),\n",
    "                           train_scores.mean(axis=1) + train_scores.std(axis=1),\n",
    "                           alpha=0.1)\n",
    "            plt.fill_between(train_sizes,\n",
    "                           val_scores.mean(axis=1) - val_scores.std(axis=1),\n",
    "                           val_scores.mean(axis=1) + val_scores.std(axis=1),\n",
    "                           alpha=0.1)\n",
    "            plt.title(f'Learning Curves - {name}')\n",
    "            plt.xlabel('Training examples')\n",
    "            plt.ylabel('Root Mean Squared Error')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "\n",
    "            # Plot residual plots\n",
    "            residuals = y_val - y_pred\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "            plt.axhline(y=0, color='r', linestyle='--')\n",
    "            plt.title(f'Residual Plot - {name}')\n",
    "            plt.xlabel('Predicted Values')\n",
    "            plt.ylabel('Residuals')\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error with {name}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    top5_models_df = results_df.sort_values(by=\"RMSE\", ascending=True).head(5).reset_index(drop=True)\n",
    "\n",
    "    return results_df, top5_models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_column_names(df):\n",
    "    df.columns = df.columns.str.replace(r\"[^\\w\\d_]+\", \"_\", regex=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    try:\n",
    "        df_dev = pd.read_csv('./output_csv/Final_Output_Reg_dev.csv')\n",
    "        X, y = split_data(df_dev)\n",
    "        X = clean_column_names(X)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        \n",
    "        \n",
    "        model_params = get_enhanced_model_params()\n",
    "        results_df, top5_models_df = evaluate_models(model_params, X_train, y_train, X_val, y_val)\n",
    "        \n",
    "        print(\"\\nTop 5 Models:\")\n",
    "        display(top5_models_df)\n",
    "        \n",
    "        results_df.to_csv('./output_csv/regression_results.csv', index=False)\n",
    "        top5_models_df.to_csv('./output_csv/top5_regression_models.csv', index=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
