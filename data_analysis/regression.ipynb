{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import learning_curve\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../build_dataset/output_csv/bloomberg_data.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_trade_date(df):\n",
    "    \"\"\"\n",
    "    Convert the 'Trade Date (US)' column to separate columns for \n",
    "    month, day, and year, and drop the original 'Trade Date (US)' column.\n",
    "    \"\"\"\n",
    "    df['Trade Date (US)'] = pd.to_datetime(df['Trade Date (US)'])\n",
    "    df['Trade Month'] = df['Trade Date (US)'].dt.month\n",
    "    df['Trade Day'] = df['Trade Date (US)'].dt.day\n",
    "    df['Trade Year'] = df['Trade Date (US)'].dt.year\n",
    "    df.drop(columns=['Trade Date (US)'], inplace=True)\n",
    "    return df\n",
    "\n",
    "df_convert_trade_date = convert_trade_date(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(columns,df):\n",
    "    \"\"\"\n",
    "    Drop the specified columns from the DataFrame.\n",
    "    \"\"\"\n",
    "    df = df.drop(columns=columns)\n",
    "    return df\n",
    "\n",
    "df_drop_columns = drop_columns(['Issuer Ticker',\n",
    "                                'Issuer Name',\n",
    "                                'Filing Term Price Range',\n",
    "                                'cusip',\n",
    "                                'Priced Range'],\n",
    "                                df_convert_trade_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding(df):\n",
    "    \"\"\"\n",
    "    Split the DataFrame into development and testing sets. \n",
    "    \n",
    "    Fill missing values with the mode for categorical \n",
    "    columns and the mean for numerical columns.\n",
    "\n",
    "    Encode columns in the DataFrame: \n",
    "\n",
    "      - Categorical columns are encoded using OneHotEncoder.\n",
    "      - Numerical columns are scaled using StandardScaler.\n",
    "      - Ordinal columns are encoded using OrdinalEncoder.\n",
    "\n",
    "    Create df_dev and df_test using the transformed features.\n",
    "    \"\"\"\n",
    "    X = df.drop(columns=['Offer To 1st Close'])\n",
    "    y = df['Offer To 1st Close']\n",
    "\n",
    "    X_dev, X_test, y_dev, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    if y_dev.isnull().any() or y_test.isnull().any():\n",
    "        mean = y_dev.mean()\n",
    "        y_dev = y_dev.fillna(mean)\n",
    "        y_test = y_test.fillna(mean)\n",
    "\n",
    "    for col in X_dev.columns:\n",
    "        if X_dev[col].dtype == 'object':\n",
    "            mode = X_dev[col].mode()[0]\n",
    "            X_dev[col] = X_dev[col].fillna(mode)\n",
    "            X_test[col] = X_test[col].fillna(mode)\n",
    "        else:\n",
    "            mean = X_dev[col].mean()\n",
    "            X_dev[col] = X_dev[col].fillna(mean)\n",
    "            X_test[col] = X_test[col].fillna(mean)\n",
    "\n",
    "    oe_columns = ['Trade Month', 'Trade Day', 'Trade Year'] \n",
    "    ohe_columns = ['Industry Sector', 'Industry Group', 'Industry Subgroup']\n",
    "    ss_columns = [col for col in X.select_dtypes(exclude=['object']).columns if col not in oe_columns]\n",
    "\n",
    "\n",
    "    oe = OrdinalEncoder()\n",
    "    ohe = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "    ss = StandardScaler()\n",
    "\n",
    "    preprocess = ColumnTransformer(transformers=[\n",
    "        ('ohe', ohe, ohe_columns),\n",
    "        ('ss', ss, ss_columns),\n",
    "        ('oe', oe, oe_columns)\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "    transformed_dev = preprocess.fit_transform(X_dev)\n",
    "    transformed_test = preprocess.transform(X_test)\n",
    "\n",
    "    features = preprocess.get_feature_names_out()\n",
    "\n",
    "    df_dev = pd.DataFrame(transformed_dev, columns=features)\n",
    "    df_test = pd.DataFrame(transformed_test, columns=features)\n",
    "\n",
    "    df_dev['Offer To 1st Close'] = y_dev.reset_index(drop=True)\n",
    "    df_test['Offer To 1st Close'] = y_test.reset_index(drop=True)\n",
    "\n",
    "    return df_dev, df_test\n",
    "\n",
    "\n",
    "df_dev_encoding, df_test_encoding = encoding(df_drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_correlation(df_dev, df_test):\n",
    "    \"\"\"\n",
    "    Check correlation in the X_dev column and drop highly correlated features.\n",
    "    \"\"\"\n",
    "    X_dev = df_dev.drop(columns=['Offer To 1st Close'])\n",
    "    y_dev = df_dev['Offer To 1st Close']\n",
    "\n",
    "    X_test = df_test.drop(columns=['Offer To 1st Close'])\n",
    "    y_test = df_test['Offer To 1st Close']\n",
    "\n",
    "    corr_matrix = X_dev.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] >= 0.9)]\n",
    "\n",
    "    X_reduced = X_dev.drop(columns=to_drop)\n",
    "    X_reduced_test = X_test.drop(columns=to_drop)\n",
    "\n",
    "    df_dev = pd.concat([X_reduced, y_dev.reset_index(drop=True)], axis=1)\n",
    "    df_test = pd.concat([X_reduced_test, y_test.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    print(f\"Removed {len(to_drop)} correlated features.\")\n",
    "\n",
    "    df_test.to_csv('./output_csv/Final_Output_Reg_test.csv', index=False)\n",
    "\n",
    "    return df_dev, df_test\n",
    "\n",
    "df_dev_correlation, df_test_correlation = show_correlation(df_dev_encoding, df_test_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outlier(df):\n",
    "    \"\"\"\n",
    "    Remove rows that correspond to outliers in the target variable\n",
    "    \"\"\"\n",
    "    label = df['Offer To 1st Close']\n",
    "    Q1 = label.quantile(0.25)\n",
    "    Q3 = label.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    df_filtered = df[(label >= lower_bound) & (label <= upper_bound)]\n",
    "    print(f\"Removed outliers.\")\n",
    "    print(f\"Original count: {len(df)}, New count: {len(df_filtered)}\")\n",
    "    return df_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkoutliers(df, outfile):\n",
    "    \"\"\"\n",
    "    Print distribution of the target variable and check for outliers.\n",
    "    \"\"\"\n",
    "    label = df['Offer To 1st Close']\n",
    "\n",
    "    Q1 = label.quantile(0.25)\n",
    "    Q3 = label.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    print('Minimum value: ', label.min())\n",
    "    print('Q1: ', Q1)\n",
    "    print('Median value: ', label.median())\n",
    "    print('Q3: ', Q3)\n",
    "    print('Maximum value: ', label.max())\n",
    "    print('Mean value: ', label.mean())\n",
    "    print('Standard deviation: ', label.std())\n",
    "\n",
    "    outliers = (label < lower_bound).any() or (label > upper_bound).any()\n",
    "\n",
    "    if outliers:\n",
    "        print('\\nThere are outliers in the data.')\n",
    "        df_filtered = remove_outlier(df)\n",
    "        df_filtered.to_csv(outfile, index=False)\n",
    "        return df_filtered\n",
    "    else:\n",
    "        print('\\nThere are no outliers in the data.')\n",
    "        return df\n",
    "\n",
    "df_dev_filtered = checkoutliers(df_dev_correlation, './output_csv/Final_Output_Reg_dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_distribution(df_dev_filtered, df_dev_correlation):\n",
    "    df_no_outliers = df_dev_filtered.copy()\n",
    "    df_with_outliers = df_dev_correlation.copy()\n",
    "\n",
    "    fig, axs = plt.subplots(ncols=2, figsize=(15, 10))\n",
    "\n",
    "    sns.boxplot(data=df_with_outliers, y='Offer To 1st Close', color='lightblue', ax=axs[0])\n",
    "    axs[0].set_title('With Outliers')\n",
    "    axs[0].set_xlabel('Offer to 1st Close')\n",
    "\n",
    "    sns.boxplot(data=df_no_outliers, y='Offer To 1st Close', color='lightblue', ax=axs[1])\n",
    "    axs[1].set_title('Removed Outliers')\n",
    "    axs[1].set_xlabel('Offer to 1st Close')\n",
    "\n",
    "    fig.suptitle('Regression Task: Distribution of Offer to 1st Close', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "display_distribution(df_dev_filtered, df_dev_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, target_col='Offer To 1st Close'):\n",
    "    \"\"\"Preprocess the data with feature engineering and scaling\"\"\"\n",
    "    X = df.drop(columns=target_col)\n",
    "    y = df[target_col]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_enhanced_model_params():\n",
    "    \"\"\"Return enhanced model parameters with wider search space\"\"\"\n",
    "    return {\n",
    "        \"Linear Regression\": {\n",
    "            \"model\": LinearRegression(n_jobs=-1),\n",
    "            \"params\": {}\n",
    "        },\n",
    "        \"Ridge Regression\": {\n",
    "            \"model\": Ridge(random_state=42, max_iter=10000),\n",
    "            \"params\": {\n",
    "                \"alpha\": [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "            }\n",
    "        },\n",
    "        \"Lasso Regression\": {\n",
    "            \"model\": Lasso(random_state=42, max_iter=10000),\n",
    "            \"params\": {\n",
    "                \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "            }\n",
    "        },\n",
    "        \"ElasticNet Regression\": {\n",
    "            \"model\": ElasticNet(random_state=42, max_iter=10000),\n",
    "            \"params\": {\n",
    "                \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10],\n",
    "                \"l1_ratio\": [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "            }\n",
    "        },\n",
    "        \"Support Vector Regressor\": {\n",
    "            \"model\": SVR(),\n",
    "            \"params\": {\n",
    "                \"kernel\": [\"rbf\", \"poly\"],\n",
    "                \"C\": [0.1, 1, 10],\n",
    "                \"gamma\": [\"scale\", \"auto\"]\n",
    "            }\n",
    "        },\n",
    "        \"LightGBM\": {\n",
    "            \"model\": LGBMRegressor(random_state=42, n_jobs=-1),\n",
    "            \"params\": {\n",
    "                \"n_estimators\": [100, 200, 500],\n",
    "                \"learning_rate\": [0.001, 0.01, 0.1],\n",
    "                \"max_depth\": [3, 5, 7, -1],\n",
    "                \"num_leaves\": [15, 31, 63],\n",
    "                \"subsample\": [0.8, 0.9, 1.0],\n",
    "                \"verbosity\": [-1]  \n",
    "            }\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(model_params, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Evaluate models with enhanced metrics and display progress\"\"\"\n",
    "    results = []\n",
    "\n",
    "    total_models = len(model_params)\n",
    "    print(f\"Evaluating {total_models} models...\\n\")\n",
    "\n",
    "    for i, (name, mp) in enumerate(model_params.items(), 1):\n",
    "        print(f\"[{i}/{total_models}] Training and evaluating: {name}...\", end=\" \")\n",
    "\n",
    "        try:\n",
    "            if mp[\"params\"]:\n",
    "                grid = GridSearchCV(\n",
    "                    mp[\"model\"], \n",
    "                    mp[\"params\"], \n",
    "                    cv=5, \n",
    "                    scoring='r2', \n",
    "                    n_jobs=-1,\n",
    "                    verbose=0\n",
    "                )\n",
    "                grid.fit(X_train, y_train)\n",
    "                best_model = grid.best_estimator_\n",
    "                print(\"✓ (GridSearch complete)\")\n",
    "            else:\n",
    "                best_model = mp[\"model\"]\n",
    "                best_model.fit(X_train, y_train)\n",
    "                print(\"✓ (No tuning needed)\")\n",
    "            \n",
    "            # Cross-validation scores\n",
    "            cv_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='r2')\n",
    "            \n",
    "            y_pred = best_model.predict(X_val)\n",
    "            r2 = r2_score(y_val, y_pred)\n",
    "            mae = mean_absolute_error(y_val, y_pred)\n",
    "            mse = mean_squared_error(y_val, y_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            \n",
    "            results.append({\n",
    "                \"Model\": name,\n",
    "                \"R² Score\": r2,\n",
    "                \"MAE\": mae,\n",
    "                \"MSE\": mse,\n",
    "                \"RMSE\": rmse,\n",
    "                \"CV R² Mean\": cv_scores.mean(),\n",
    "                \"CV R² Std\": cv_scores.std()\n",
    "            })\n",
    "            \n",
    "            # Plot learning curves\n",
    "            train_sizes, train_scores, val_scores = learning_curve(\n",
    "                best_model, X_train, y_train, cv=5, scoring='r2',\n",
    "                train_sizes=np.linspace(0.1, 1.0, 10)\n",
    "            )\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(train_sizes, train_scores.mean(axis=1), label='Training score')\n",
    "            plt.plot(train_sizes, val_scores.mean(axis=1), label='Cross-validation score')\n",
    "            plt.fill_between(train_sizes, \n",
    "                           train_scores.mean(axis=1) - train_scores.std(axis=1),\n",
    "                           train_scores.mean(axis=1) + train_scores.std(axis=1),\n",
    "                           alpha=0.1)\n",
    "            plt.fill_between(train_sizes,\n",
    "                           val_scores.mean(axis=1) - val_scores.std(axis=1),\n",
    "                           val_scores.mean(axis=1) + val_scores.std(axis=1),\n",
    "                           alpha=0.1)\n",
    "            plt.title(f'Learning Curves - {name}')\n",
    "            plt.xlabel('Training examples')\n",
    "            plt.ylabel('R² Score')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "            \n",
    "            # Plot feature importance for tree-based models, if applicable\n",
    "            if hasattr(best_model, 'feature_importances_'):\n",
    "                importances = best_model.feature_importances_\n",
    "                indices = np.argsort(importances)[::-1]\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.title(f'Feature Importances - {name}')\n",
    "                plt.bar(range(X_train.shape[1]), importances[indices])\n",
    "                plt.xticks(range(X_train.shape[1]), X_train.columns[indices], rotation=90)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error with {name}: {str(e)}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_column_names(df):\n",
    "    df.columns = df.columns.str.replace(r\"[^\\w\\d_]+\", \"_\", regex=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    try:\n",
    "        df_dev = pd.read_csv('./output_csv/Final_Output_Reg_dev.csv')\n",
    "        X, y = split_data(df_dev)\n",
    "        X = clean_column_names(X)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        model_params = get_enhanced_model_params()\n",
    "        results_df, top5_models_df = evaluate_models(model_params, X_train, y_train, X_val, y_val)\n",
    "        \n",
    "        print(\"\\nTop 5 Models:\")\n",
    "        print(top5_models_df)\n",
    "        \n",
    "        results_df.to_csv('./output_csv/regression_results.csv', index=False)\n",
    "        top5_models_df.to_csv('./output_csv/top5_regression_models.csv', index=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
