{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from custom_transformers import LogTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Issuer Ticker</th>\n",
       "      <th>Issuer Name</th>\n",
       "      <th>Offer Size (M)</th>\n",
       "      <th>Sales - 1 Yr Growth</th>\n",
       "      <th>Profit Margin</th>\n",
       "      <th>Return on Assets</th>\n",
       "      <th>Shares Outstanding (M)</th>\n",
       "      <th>Offer Price</th>\n",
       "      <th>Offer To 1st Close</th>\n",
       "      <th>Market Cap at Offer (M)</th>\n",
       "      <th>...</th>\n",
       "      <th>Filing Term Price Range</th>\n",
       "      <th>Priced Range</th>\n",
       "      <th>Industry Group</th>\n",
       "      <th>Industry Sector</th>\n",
       "      <th>Industry Subgroup</th>\n",
       "      <th>Instit Owner (% Shares Out)</th>\n",
       "      <th>Fed Rate</th>\n",
       "      <th>CPI</th>\n",
       "      <th>Consumer Confidence</th>\n",
       "      <th>Unemployment Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AWE US</td>\n",
       "      <td>New Cingular Wireless Services Inc</td>\n",
       "      <td>10620.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.075987</td>\n",
       "      <td>360.0</td>\n",
       "      <td>29.5</td>\n",
       "      <td>7.838983</td>\n",
       "      <td>10620.0</td>\n",
       "      <td>...</td>\n",
       "      <td>26.00 - 32.00</td>\n",
       "      <td>Priced Within Range</td>\n",
       "      <td>Telecommunications</td>\n",
       "      <td>Communications</td>\n",
       "      <td>Cellular Telecom</td>\n",
       "      <td>0.000341</td>\n",
       "      <td>6.019667</td>\n",
       "      <td>180.3</td>\n",
       "      <td>109.2</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3527578Q US</td>\n",
       "      <td>Agere Systems Inc</td>\n",
       "      <td>4140.00</td>\n",
       "      <td>-6.3</td>\n",
       "      <td>1.2</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>600.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.00 - 7.00</td>\n",
       "      <td>Priced Within Range</td>\n",
       "      <td>Semiconductors</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Electronic Compo-Semicon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.310968</td>\n",
       "      <td>184.7</td>\n",
       "      <td>91.5</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BGPTQ US</td>\n",
       "      <td>BearingPoint Inc</td>\n",
       "      <td>2328.38</td>\n",
       "      <td>-7.4</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.700000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>30.468750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>16.00 - 18.00</td>\n",
       "      <td>Priced Within Range</td>\n",
       "      <td>Commercial Services</td>\n",
       "      <td>Consumer, Non-cyclical</td>\n",
       "      <td>Consulting Services</td>\n",
       "      <td>0.009101</td>\n",
       "      <td>5.494286</td>\n",
       "      <td>184.4</td>\n",
       "      <td>90.6</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Issuer Ticker                         Issuer Name  Offer Size (M)  \\\n",
       "0        AWE US  New Cingular Wireless Services Inc        10620.00   \n",
       "1   3527578Q US                   Agere Systems Inc         4140.00   \n",
       "2      BGPTQ US                    BearingPoint Inc         2328.38   \n",
       "\n",
       "   Sales - 1 Yr Growth  Profit Margin  Return on Assets  \\\n",
       "0                  NaN            NaN          0.075987   \n",
       "1                 -6.3            1.2          3.300000   \n",
       "2                 -7.4           -1.0         -1.700000   \n",
       "\n",
       "   Shares Outstanding (M)  Offer Price  Offer To 1st Close  \\\n",
       "0                   360.0         29.5            7.838983   \n",
       "1                   600.0          6.0            0.333333   \n",
       "2                     0.0         18.0           30.468750   \n",
       "\n",
       "   Market Cap at Offer (M)  ... Filing Term Price Range         Priced Range  \\\n",
       "0                  10620.0  ...           26.00 - 32.00  Priced Within Range   \n",
       "1                   3600.0  ...             6.00 - 7.00  Priced Within Range   \n",
       "2                      0.0  ...           16.00 - 18.00  Priced Within Range   \n",
       "\n",
       "        Industry Group         Industry Sector         Industry Subgroup  \\\n",
       "0   Telecommunications          Communications          Cellular Telecom   \n",
       "1       Semiconductors              Technology  Electronic Compo-Semicon   \n",
       "2  Commercial Services  Consumer, Non-cyclical       Consulting Services   \n",
       "\n",
       "  Instit Owner (% Shares Out)  Fed Rate    CPI Consumer Confidence  \\\n",
       "0                    0.000341  6.019667  180.3               109.2   \n",
       "1                         NaN  5.310968  184.7                91.5   \n",
       "2                    0.009101  5.494286  184.4                90.6   \n",
       "\n",
       "   Unemployment Rate  \n",
       "0                3.8  \n",
       "1                4.3  \n",
       "2                4.2  \n",
       "\n",
       "[3 rows x 24 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.makedirs('./output_csv', exist_ok=True)\n",
    "df = pd.read_csv('../build_dataset/output_csv/bloomberg_data.csv')\n",
    "df = df.dropna(subset=['Offer To 1st Close'])\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rolling_ipos(df):\n",
    "    df['Trade Date (US)'] = pd.to_datetime(df['Trade Date (US)'])\n",
    "    df = df.sort_values('Trade Date (US)')\n",
    "    df['Prev_5_IPOs_Avg_Return'] = df['Offer To 1st Close'].rolling(window=5, min_periods=1).mean().shift(1)\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_rolling_ipos = add_rolling_ipos(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_trade_date(df):\n",
    "    \"\"\"\n",
    "    Convert the 'Trade Date (US)' column to separate columns for \n",
    "    month, day, and year, and drop the original 'Trade Date (US)' column.\n",
    "    \"\"\"\n",
    "    df['Trade Date (US)'] = pd.to_datetime(df['Trade Date (US)'])\n",
    "    df['Trade Month'] = df['Trade Date (US)'].dt.month\n",
    "    df['Trade Day'] = df['Trade Date (US)'].dt.day\n",
    "    df['Trade Year'] = df['Trade Date (US)'].dt.year\n",
    "    df.drop(columns=['Trade Date (US)'], inplace=True)\n",
    "    return df\n",
    "\n",
    "df_convert_trade_date = convert_trade_date(df_rolling_ipos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(columns,df):\n",
    "    \"\"\"\n",
    "    Drop the specified columns from the DataFrame.\n",
    "    \"\"\"\n",
    "    df = df.drop(columns=columns)\n",
    "    return df\n",
    "\n",
    "df_drop_columns = drop_columns(['Issuer Ticker',\n",
    "                                'Issuer Name',\n",
    "                                'Filing Term Price Range',\n",
    "                                'cusip',\n",
    "                                'Priced Range'],\n",
    "                                df_convert_trade_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding(df):\n",
    "    \"\"\"\n",
    "    Split the DataFrame into development and testing sets and encode the columns.\n",
    "    \"\"\"\n",
    "\n",
    "    X = df.drop(columns=['Offer To 1st Close'])\n",
    "    y = df['Offer To 1st Close']\n",
    "    \n",
    "    X_dev, X_test, y_dev, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_dev, y_dev, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "    # FILL MISSING VALUES\n",
    "    for col in X_train.columns:\n",
    "        if col in ['Trade Month', 'Trade Day', 'Trade Year']:\n",
    "            X_train[col] = X_train[col].fillna(X_train[col].mode()[0])\n",
    "            X_val[col] = X_val[col].fillna(X_train[col].mode()[0])\n",
    "            X_test[col] = X_test[col].fillna(X_train[col].mode()[0])\n",
    "        elif X_train[col].dtype == 'object':\n",
    "            X_train[col] = X_train[col].fillna(X_train[col].mode()[0])\n",
    "            X_val[col] = X_val[col].fillna(X_train[col].mode()[0])\n",
    "            X_test[col] = X_test[col].fillna(X_train[col].mode()[0])\n",
    "        else:\n",
    "            X_train[col] = X_train[col].fillna(X_train[col].mean())\n",
    "            X_val[col] = X_val[col].fillna(X_train[col].mean())\n",
    "            X_test[col] = X_test[col].fillna(X_test[col].mean())\n",
    "\n",
    "\n",
    "    # ENCODE TARGET VARIABLE\n",
    "    yeo = PowerTransformer(method='yeo-johnson')\n",
    "    y_train = yeo.fit_transform(y_train.values.reshape(-1, 1)).flatten()\n",
    "    y_val = yeo.transform(y_val.values.reshape(-1, 1)).flatten()\n",
    "    y_test = yeo.transform(y_test.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # ENCODING FEATURES\n",
    "    oe_columns = ['Trade Month', 'Trade Day', 'Trade Year']\n",
    "    ohe_columns = ['Industry Sector', 'Industry Group', 'Industry Subgroup']\n",
    "    log_columns = ['Offer Price', 'Market Cap at Offer (M)']\n",
    "    exclude_columns = oe_columns + log_columns + ohe_columns\n",
    "    ss_columns = [col for col in X.select_dtypes(exclude=['object']).columns if col not in exclude_columns]\n",
    "\n",
    "    oe = OrdinalEncoder()\n",
    "    log = LogTransformer()\n",
    "    ohe = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "    ss = StandardScaler()\n",
    "\n",
    "    preprocess = ColumnTransformer(transformers=[\n",
    "        ('ohe', ohe, ohe_columns),\n",
    "        ('log', log, log_columns),\n",
    "        ('ss', ss, ss_columns),\n",
    "        ('oe', oe, oe_columns)\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "    transformed_train = preprocess.fit_transform(X_train)\n",
    "    transformed_val = preprocess.transform(X_val)\n",
    "    transformed_test = preprocess.transform(X_test)\n",
    "\n",
    "    features = preprocess.get_feature_names_out()\n",
    "\n",
    "    df_train = pd.DataFrame(transformed_train, columns=features)\n",
    "    df_val = pd.DataFrame(transformed_val, columns=features)\n",
    "    df_test = pd.DataFrame(transformed_test, columns=features)\n",
    "\n",
    "    df_train['Offer To 1st Close'] = y_train\n",
    "    df_val['Offer To 1st Close'] = y_val\n",
    "    df_test['Offer To 1st Close'] = y_test\n",
    "\n",
    "    return df_train, df_val, df_test, yeo, preprocess\n",
    "\n",
    "\n",
    "df_train_encoded, df_val_encoded, df_test_encoded, yeo, preprocess = encoding(df_drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Importances:\n",
      "log__Offer Price                   0.243283\n",
      "ss__Offer Size (M)                 0.104284\n",
      "log__Market Cap at Offer (M)       0.055430\n",
      "ss__Shares Outstanding (M)         0.048082\n",
      "ss__Instit Owner (% Shares Out)    0.042401\n",
      "ss__Sales - 1 Yr Growth            0.040779\n",
      "ss__Prev_5_IPOs_Avg_Return         0.034605\n",
      "ss__Return on Assets               0.030186\n",
      "ss__Profit Margin                  0.029869\n",
      "ss__Cash Flow per Share            0.028547\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def show_feature_importance(df):\n",
    "    \"\"\"\n",
    "    Calculate feature importance using a simple model\n",
    "    \"\"\"\n",
    "    X_train = df.drop(columns=['Offer To 1st Close'])\n",
    "    y_train = df['Offer To 1st Close']\n",
    "    \n",
    "    rf = RandomForestRegressor(random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    importances = pd.Series(rf.feature_importances_, index=X_train.columns)\n",
    "    print(\"\\nFeature Importances:\")\n",
    "    print(importances.sort_values(ascending=False).head(10))\n",
    "\n",
    "show_feature_importance(df_train_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correlation(df_train, df_val, df_test):\n",
    "    \"\"\"\n",
    "    Check correlation in the X_dev column and drop highly correlated features.\n",
    "    \"\"\"\n",
    "    X_train = df_train.drop(columns=['Offer To 1st Close'])\n",
    "    y_train = df_train['Offer To 1st Close']\n",
    "\n",
    "    X_val = df_val.drop(columns=['Offer To 1st Close'])\n",
    "    y_val = df_val['Offer To 1st Close']\n",
    "\n",
    "    X_test = df_test.drop(columns=['Offer To 1st Close'])\n",
    "    y_test = df_test['Offer To 1st Close']\n",
    "\n",
    "    corr_matrix = X_train.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] >= 0.9)]\n",
    "\n",
    "    X_reduced_train = X_train.drop(columns=to_drop)\n",
    "    X_reduced_val = X_val.drop(columns=to_drop)\n",
    "    X_reduced_test = X_test.drop(columns=to_drop)\n",
    "\n",
    "    df_train = pd.concat([X_reduced_train, y_train.reset_index(drop=True)], axis=1)\n",
    "    df_val = pd.concat([X_reduced_val, y_val.reset_index(drop=True)], axis=1)\n",
    "    df_test = pd.concat([X_reduced_test, y_test.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    print(f\"Removed {len(to_drop)} correlated features.\")\n",
    "\n",
    "    df_train.to_csv('./output_csv/Final_Output_Reg_train.csv', index=False)\n",
    "    df_val.to_csv('./output_csv/Final_Output_Reg_val.csv', index=False)\n",
    "    df_test.to_csv('./output_csv/Final_Output_Reg_test.csv', index=False)\n",
    "\n",
    "    return df_train, df_val, df_test\n",
    "\n",
    "df_train_correlation, df_val_correlation, df_test_correlation = get_correlation(df_train_encoded, df_val_encoded, df_test_encoded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outlier(df):\n",
    "    \"\"\"\n",
    "    Remove outliers using IQR on target variable.\n",
    "    \"\"\"\n",
    "    label = df['Offer To 1st Close']\n",
    "    Q1 = label.quantile(0.25)\n",
    "    Q3 = label.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    df_filtered = df[(label >= lower_bound) & (label <= upper_bound)]\n",
    "    \n",
    "    print(f\"Removed outliers.\")\n",
    "    print(f\"Original count: {len(df)}\")\n",
    "    print(f\"After IQR filtering: {len(df_filtered)}\")\n",
    "    \n",
    "    return df_filtered, (lower_bound, upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkoutliers(df, outfile):\n",
    "    \"\"\"\n",
    "    Print distribution of the target variable and check for outliers.\n",
    "    \"\"\"\n",
    "    label = df['Offer To 1st Close']\n",
    "\n",
    "    df_stat = pd.DataFrame([{\n",
    "        'Minimum': label.min(),\n",
    "        'Q1': label.quantile(0.25),\n",
    "        'Median': label.median(),\n",
    "        'Q3': label.quantile(0.75),\n",
    "        'Maximum': label.max(),\n",
    "        'Mean': label.mean(),\n",
    "        'Standard deviation': label.std()\n",
    "    }])\n",
    "\n",
    "    display(df_stat.transpose().rename(columns={0: 'Values'}))\n",
    "    print('\\nChecking for outliers...')\n",
    "    \n",
    "    df_filtered, bounds = remove_outlier(df)\n",
    "    df_filtered.to_csv(outfile, index=False)\n",
    "    return df_filtered, bounds\n",
    "\n",
    "df_dev_filtered, bounds = checkoutliers(df_train_correlation, './output_csv/Final_Output_Reg_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_distribution(df_dev_filtered, df_dev_correlation):\n",
    "    \"\"\"\n",
    "    Display distribution of target variable before and after removing outliers.\n",
    "    \"\"\"\n",
    "    df_no_outliers = df_dev_filtered.copy()\n",
    "    df_with_outliers = df_dev_correlation.copy()\n",
    "\n",
    "    fig, axs = plt.subplots(ncols=2, figsize=(15, 10))\n",
    "\n",
    "    sns.boxplot(data=df_with_outliers, y='Offer To 1st Close', color='lightblue', ax=axs[0])\n",
    "    axs[0].set_title('With Outliers')\n",
    "    axs[0].set_xlabel('Offer to 1st Close')\n",
    "\n",
    "    sns.boxplot(data=df_no_outliers, y='Offer To 1st Close', color='lightblue', ax=axs[1])\n",
    "    axs[1].set_title('Without Outliers')\n",
    "    axs[1].set_xlabel('Offer to 1st Close')\n",
    "\n",
    "    fig.suptitle('Regression Task: Distribution of Offer to 1st Close', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "display_distribution(df_dev_filtered, df_dev_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_dimensionality_reduction(df_dev, df_test, n_components=0.95):\n",
    "    \"\"\"\n",
    "    Apply dimensionality reduction techniques to the data.\n",
    "    \"\"\"\n",
    "    X_dev = df_dev.drop(columns=['Offer To 1st Close'])\n",
    "    y_dev = df_dev['Offer To 1st Close']\n",
    "    \n",
    "    X_test = df_test.drop(columns=['Offer To 1st Close'])\n",
    "    y_test = df_test['Offer To 1st Close']\n",
    "    \n",
    "    pca = PCA(n_components=n_components, random_state=42)\n",
    "    X_dev_pca = pca.fit_transform(X_dev)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "\n",
    "    print(f\"\\nNumber of components after PCA: {pca.n_components_}\")\n",
    "    print(f\"Explained variance ratio: {pca.explained_variance_ratio_.sum():.2f}\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.title('PCA Explained Variance')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    df_dev_reduced = pd.DataFrame(X_dev_pca, columns=[f'PC{i+1}' for i in range(X_dev_pca.shape[1])])\n",
    "    df_test_reduced = pd.DataFrame(X_test_pca, columns=[f'PC{i+1}' for i in range(X_test_pca.shape[1])])\n",
    "    \n",
    "    df_dev_reduced['Offer To 1st Close'] = y_dev.reset_index(drop=True)\n",
    "    df_test_reduced['Offer To 1st Close'] = y_test.reset_index(drop=True)\n",
    "    \n",
    "    df_dev_reduced.to_csv('./output_csv/Final_Output_Reg_dev.csv', index=False)\n",
    "    df_test_reduced.to_csv('./output_csv/Final_Output_Reg_test.csv', index=False)\n",
    "    \n",
    "    return df_dev_reduced, df_test_reduced\n",
    "\n",
    "df_dev_reduced, df_test_reduced = apply_dimensionality_reduction(df_dev_filtered, df_test_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, target_col='Offer To 1st Close'):\n",
    "    \"\"\"\n",
    "    Preprocess the data with feature engineering and scaling\n",
    "    \"\"\"\n",
    "    X = df.drop(columns=target_col)\n",
    "    y = df[target_col]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_enhanced_model_params():\n",
    "    \"\"\"Return enhanced model parameters with wider search space\"\"\"\n",
    "    return {\n",
    "        \"Dummy Regressor\": {\n",
    "            \"model\": DummyRegressor(strategy='mean'),\n",
    "            \"params\": {}\n",
    "        },\n",
    "        \"Linear Regression\": {\n",
    "            \"model\": LinearRegression(n_jobs=-1),\n",
    "            \"params\": {}\n",
    "        },\n",
    "        \"Ridge Regression\": {\n",
    "            \"model\": Ridge(random_state=42, max_iter=10000),\n",
    "            \"params\": {\n",
    "                \"alpha\": [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "            }\n",
    "        },\n",
    "        \"Lasso Regression\": {\n",
    "            \"model\": Lasso(random_state=42, max_iter=10000),\n",
    "            \"params\": {\n",
    "                \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "            }\n",
    "        },\n",
    "        \"ElasticNet Regression\": {\n",
    "            \"model\": ElasticNet(random_state=42, max_iter=10000),\n",
    "            \"params\": {\n",
    "                \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10],\n",
    "                \"l1_ratio\": [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "            }\n",
    "        },\n",
    "        \"LightGBM\": {\n",
    "            \"model\": LGBMRegressor(\n",
    "                random_state=42,\n",
    "                force_col_wise=True,  \n",
    "                min_data_in_leaf=10,  \n",
    "                feature_fraction=0.8,  \n",
    "                bagging_fraction=0.8, \n",
    "                bagging_freq=1,     \n",
    "                verbose=-1         \n",
    "            ),\n",
    "            \"params\": {\n",
    "                \"n_estimators\": [100, 200],\n",
    "                \"max_depth\": [5, 7],\n",
    "                \"learning_rate\": [0.01, 0.05],\n",
    "                \"num_leaves\": [20, 31],\n",
    "                \"reg_alpha\": [0.01, 0.1],     \n",
    "                \"reg_lambda\": [0.01, 0.1]\n",
    "            }\n",
    "        },\n",
    "        \"Random Forest (Light)\": {\n",
    "            \"model\": RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "            \"params\": {\n",
    "                \"n_estimators\": [50, 100],\n",
    "                \"max_depth\": [3, 5],\n",
    "                \"min_samples_split\": [5, 10]\n",
    "            }\n",
    "        },\n",
    "        \"Gradient Boosting\": {\n",
    "            \"model\": GradientBoostingRegressor(random_state=42),\n",
    "            \"params\": {\n",
    "                \"n_estimators\": [50, 100],\n",
    "                \"max_depth\": [3, 5],\n",
    "                \"learning_rate\": [0.01, 0.05],\n",
    "                \"subsample\": [0.8, 1.0],\n",
    "                \"min_samples_split\": [5, 10]\n",
    "            }\n",
    "        },\n",
    "        \"Extra Trees\": {\n",
    "            \"model\": ExtraTreesRegressor(random_state=42, n_jobs=-1),\n",
    "            \"params\": {\n",
    "                \"n_estimators\": [50, 100],\n",
    "                \"max_depth\": [3, 5],\n",
    "                \"min_samples_split\": [5, 10]\n",
    "            }\n",
    "        },\n",
    "        \"Kernel Ridge\": {\n",
    "            \"model\": KernelRidge(kernel='rbf'),\n",
    "            \"params\": {\n",
    "                \"alpha\": [0.001, 0.01, 0.1, 1.0],\n",
    "                \"gamma\": [0.001, 0.01, 0.1, 1.0]\n",
    "            }\n",
    "        },\n",
    "        \"Neural Network\": {\n",
    "            \"model\": MLPRegressor(\n",
    "                random_state=42,\n",
    "                max_iter=2000,          \n",
    "                early_stopping=True,    \n",
    "                validation_fraction=0.1, \n",
    "                n_iter_no_change=50,    \n",
    "                batch_size='auto',      \n",
    "                learning_rate='adaptive' \n",
    "            ),\n",
    "            \"params\": {\n",
    "                \"hidden_layer_sizes\": [(100,), (100, 50), (50, 25)],\n",
    "                \"learning_rate_init\": [0.001, 0.005],\n",
    "                \"alpha\": [0.0001, 0.001, 0.01],\n",
    "                \"activation\": ['relu', 'tanh']\n",
    "            }\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(model_params, X_train, y_train, X_val, y_val, yeo):\n",
    "    \"\"\"Evaluate models with RMSE as the primary metric\"\"\"\n",
    "    results = []\n",
    "\n",
    "    total_models = len(model_params)\n",
    "    print(f\"Evaluating {total_models} models...\\n\")\n",
    "\n",
    "    for i, (name, mp) in enumerate(model_params.items(), 1):\n",
    "        print(f\"[{i}/{total_models}] Training and evaluating: {name}...\", end=\" \")\n",
    "\n",
    "        try:\n",
    "            if mp[\"params\"]:\n",
    "                grid = GridSearchCV(\n",
    "                    mp[\"model\"],\n",
    "                    mp[\"params\"],\n",
    "                    cv=5,\n",
    "                    scoring='neg_root_mean_squared_error',\n",
    "                    n_jobs=-1,\n",
    "                    verbose=0\n",
    "                )\n",
    "                grid.fit(X_train, y_train)\n",
    "                best_model = grid.best_estimator_\n",
    "                print(\"✓ (GridSearch complete)\")\n",
    "            else:\n",
    "                best_model = mp[\"model\"]\n",
    "                best_model.fit(X_train, y_train)\n",
    "                print(\"✓ (No tuning needed)\")\n",
    "\n",
    "            cv_scores = cross_val_score(best_model, X_train, y_train, cv=5,\n",
    "                                      scoring='neg_root_mean_squared_error')\n",
    "            cv_scores = -cv_scores\n",
    "\n",
    "            y_pred = best_model.predict(X_val)\n",
    "            y_pred = yeo.inverse_transform(y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "            mae = mean_absolute_error(y_val, y_pred)\n",
    "            r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "            results.append({\n",
    "                \"Model\": name,\n",
    "                \"RMSE\": rmse,\n",
    "                \"MAE\": mae,\n",
    "                \"R² Score\": r2,\n",
    "                \"CV RMSE Mean\": cv_scores.mean(),\n",
    "                \"CV RMSE Std\": cv_scores.std()\n",
    "            })\n",
    "\n",
    "            train_sizes, train_scores, val_scores = learning_curve(\n",
    "                best_model, X_train, y_train, cv=5,\n",
    "                scoring='neg_root_mean_squared_error',\n",
    "                train_sizes=np.linspace(0.1, 1.0, 10)\n",
    "            )\n",
    "\n",
    "            train_scores = -train_scores\n",
    "            val_scores = -val_scores\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(train_sizes, train_scores.mean(axis=1), label='Training RMSE')\n",
    "            plt.plot(train_sizes, val_scores.mean(axis=1), label='Cross-validation RMSE')\n",
    "            plt.fill_between(train_sizes,\n",
    "                           train_scores.mean(axis=1) - train_scores.std(axis=1),\n",
    "                           train_scores.mean(axis=1) + train_scores.std(axis=1),\n",
    "                           alpha=0.1)\n",
    "            plt.fill_between(train_sizes,\n",
    "                           val_scores.mean(axis=1) - val_scores.std(axis=1),\n",
    "                           val_scores.mean(axis=1) + val_scores.std(axis=1),\n",
    "                           alpha=0.1)\n",
    "            plt.title(f'Learning Curves - {name}')\n",
    "            plt.xlabel('Training examples')\n",
    "            plt.ylabel('Root Mean Squared Error')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "\n",
    "            # Plot residual plots\n",
    "            residuals = y_val - y_pred\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "            plt.axhline(y=0, color='r', linestyle='--')\n",
    "            plt.title(f'Residual Plot - {name}')\n",
    "            plt.xlabel('Predicted Values')\n",
    "            plt.ylabel('Residuals')\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error with {name}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    top5_models_df = results_df.sort_values(by=\"RMSE\", ascending=True).head(5).reset_index(drop=True)\n",
    "\n",
    "    return results_df, top5_models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_column_names(df):\n",
    "    df.columns = df.columns.str.replace(r\"[^\\w\\d_]+\", \"_\", regex=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_best_model(model_params, X, y, top5_models_df):\n",
    "    \"\"\"\n",
    "    Evaluate the best model on the test set\n",
    "    \"\"\"\n",
    "    df_test = pd.read_csv('./output_csv/Final_Output_Reg_test.csv')\n",
    "    X_test, y_test = split_data(df_test)\n",
    "    X_test = clean_column_names(X_test)\n",
    "\n",
    "    X_test = preprocess.transform(X_test)\n",
    "    y_test = yeo.transform(y_test)\n",
    "\n",
    "    best_model_name = top5_models_df.iloc[0]['Model']\n",
    "    best_model = model_params[best_model_name]['model']\n",
    "    best_model.fit(X, y)\n",
    "\n",
    "    dummy_model = model_params['Dummy Regressor']['model']\n",
    "    dummy_model.fit(X, y)\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_pred_dummy = dummy_model.predict(X_test)\n",
    "\n",
    "    test_metrics = {\n",
    "        \"Model\": best_model_name,\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "        \"MAE\": mean_absolute_error(y_test, y_pred),\n",
    "        \"R² Score\": r2_score(y_test, y_pred),\n",
    "        \"Dummy RMSE\": np.sqrt(mean_squared_error(y_test, y_pred_dummy)),\n",
    "        \"Dummy MAE\": mean_absolute_error(y_test, y_pred_dummy),\n",
    "        \"Dummy R² Score\": r2_score(y_test, y_pred_dummy)\n",
    "    }\n",
    "\n",
    "    metrics_df = pd.DataFrame([test_metrics])\n",
    "    metrics_df.to_csv('./output_csv/best_model_Reg_test_results.csv', index=False)\n",
    "\n",
    "    print(\"\\nTest Set Performance:\")\n",
    "    display(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    try:\n",
    "        df_dev = pd.read_csv('./output_csv/Final_Output_Reg_dev.csv')\n",
    "        X, y = split_data(df_dev)\n",
    "        X = clean_column_names(X)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        model_params = get_enhanced_model_params()\n",
    "        results_df, top5_models_df = evaluate_models(model_params, X_train, y_train, X_val, y_val, yeo)\n",
    "\n",
    "        print(\"\\nTop 5 Models:\")\n",
    "        display(top5_models_df)\n",
    "\n",
    "        results_df.to_csv('./output_csv/regression_results.csv', index=False)\n",
    "        top5_models_df.to_csv('./output_csv/top5_regression_models.csv', index=False)\n",
    "\n",
    "\n",
    "        evaluate_best_model(model_params, X, y, top5_models_df)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
